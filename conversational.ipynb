{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "207a7ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import io\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85a4de82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2150f73bb06e4d1d8bfb1b1295f8eb99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/622 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c19d7628ddaa460c84e2f717b22c1c0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/3.14G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('ai-forever/rugpt3large_based_on_gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "571da848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.wte.weight\n",
      "transformer.wpe.weight\n",
      "transformer.h.0.ln_1.weight\n",
      "transformer.h.0.ln_1.bias\n",
      "transformer.h.0.attn.c_attn.weight\n",
      "transformer.h.0.attn.c_attn.bias\n",
      "transformer.h.0.attn.c_proj.weight\n",
      "transformer.h.0.attn.c_proj.bias\n",
      "transformer.h.0.ln_2.weight\n",
      "transformer.h.0.ln_2.bias\n",
      "transformer.h.0.mlp.c_fc.weight\n",
      "transformer.h.0.mlp.c_fc.bias\n",
      "transformer.h.0.mlp.c_proj.weight\n",
      "transformer.h.0.mlp.c_proj.bias\n",
      "transformer.h.1.ln_1.weight\n",
      "transformer.h.1.ln_1.bias\n",
      "transformer.h.1.attn.c_attn.weight\n",
      "transformer.h.1.attn.c_attn.bias\n",
      "transformer.h.1.attn.c_proj.weight\n",
      "transformer.h.1.attn.c_proj.bias\n",
      "transformer.h.1.ln_2.weight\n",
      "transformer.h.1.ln_2.bias\n",
      "transformer.h.1.mlp.c_fc.weight\n",
      "transformer.h.1.mlp.c_fc.bias\n",
      "transformer.h.1.mlp.c_proj.weight\n",
      "transformer.h.1.mlp.c_proj.bias\n",
      "transformer.h.2.ln_1.weight\n",
      "transformer.h.2.ln_1.bias\n",
      "transformer.h.2.attn.c_attn.weight\n",
      "transformer.h.2.attn.c_attn.bias\n",
      "transformer.h.2.attn.c_proj.weight\n",
      "transformer.h.2.attn.c_proj.bias\n",
      "transformer.h.2.ln_2.weight\n",
      "transformer.h.2.ln_2.bias\n",
      "transformer.h.2.mlp.c_fc.weight\n",
      "transformer.h.2.mlp.c_fc.bias\n",
      "transformer.h.2.mlp.c_proj.weight\n",
      "transformer.h.2.mlp.c_proj.bias\n",
      "transformer.h.3.ln_1.weight\n",
      "transformer.h.3.ln_1.bias\n",
      "transformer.h.3.attn.c_attn.weight\n",
      "transformer.h.3.attn.c_attn.bias\n",
      "transformer.h.3.attn.c_proj.weight\n",
      "transformer.h.3.attn.c_proj.bias\n",
      "transformer.h.3.ln_2.weight\n",
      "transformer.h.3.ln_2.bias\n",
      "transformer.h.3.mlp.c_fc.weight\n",
      "transformer.h.3.mlp.c_fc.bias\n",
      "transformer.h.3.mlp.c_proj.weight\n",
      "transformer.h.3.mlp.c_proj.bias\n",
      "transformer.h.4.ln_1.weight\n",
      "transformer.h.4.ln_1.bias\n",
      "transformer.h.4.attn.c_attn.weight\n",
      "transformer.h.4.attn.c_attn.bias\n",
      "transformer.h.4.attn.c_proj.weight\n",
      "transformer.h.4.attn.c_proj.bias\n",
      "transformer.h.4.ln_2.weight\n",
      "transformer.h.4.ln_2.bias\n",
      "transformer.h.4.mlp.c_fc.weight\n",
      "transformer.h.4.mlp.c_fc.bias\n",
      "transformer.h.4.mlp.c_proj.weight\n",
      "transformer.h.4.mlp.c_proj.bias\n",
      "transformer.h.5.ln_1.weight\n",
      "transformer.h.5.ln_1.bias\n",
      "transformer.h.5.attn.c_attn.weight\n",
      "transformer.h.5.attn.c_attn.bias\n",
      "transformer.h.5.attn.c_proj.weight\n",
      "transformer.h.5.attn.c_proj.bias\n",
      "transformer.h.5.ln_2.weight\n",
      "transformer.h.5.ln_2.bias\n",
      "transformer.h.5.mlp.c_fc.weight\n",
      "transformer.h.5.mlp.c_fc.bias\n",
      "transformer.h.5.mlp.c_proj.weight\n",
      "transformer.h.5.mlp.c_proj.bias\n",
      "transformer.h.6.ln_1.weight\n",
      "transformer.h.6.ln_1.bias\n",
      "transformer.h.6.attn.c_attn.weight\n",
      "transformer.h.6.attn.c_attn.bias\n",
      "transformer.h.6.attn.c_proj.weight\n",
      "transformer.h.6.attn.c_proj.bias\n",
      "transformer.h.6.ln_2.weight\n",
      "transformer.h.6.ln_2.bias\n",
      "transformer.h.6.mlp.c_fc.weight\n",
      "transformer.h.6.mlp.c_fc.bias\n",
      "transformer.h.6.mlp.c_proj.weight\n",
      "transformer.h.6.mlp.c_proj.bias\n",
      "transformer.h.7.ln_1.weight\n",
      "transformer.h.7.ln_1.bias\n",
      "transformer.h.7.attn.c_attn.weight\n",
      "transformer.h.7.attn.c_attn.bias\n",
      "transformer.h.7.attn.c_proj.weight\n",
      "transformer.h.7.attn.c_proj.bias\n",
      "transformer.h.7.ln_2.weight\n",
      "transformer.h.7.ln_2.bias\n",
      "transformer.h.7.mlp.c_fc.weight\n",
      "transformer.h.7.mlp.c_fc.bias\n",
      "transformer.h.7.mlp.c_proj.weight\n",
      "transformer.h.7.mlp.c_proj.bias\n",
      "transformer.h.8.ln_1.weight\n",
      "transformer.h.8.ln_1.bias\n",
      "transformer.h.8.attn.c_attn.weight\n",
      "transformer.h.8.attn.c_attn.bias\n",
      "transformer.h.8.attn.c_proj.weight\n",
      "transformer.h.8.attn.c_proj.bias\n",
      "transformer.h.8.ln_2.weight\n",
      "transformer.h.8.ln_2.bias\n",
      "transformer.h.8.mlp.c_fc.weight\n",
      "transformer.h.8.mlp.c_fc.bias\n",
      "transformer.h.8.mlp.c_proj.weight\n",
      "transformer.h.8.mlp.c_proj.bias\n",
      "transformer.h.9.ln_1.weight\n",
      "transformer.h.9.ln_1.bias\n",
      "transformer.h.9.attn.c_attn.weight\n",
      "transformer.h.9.attn.c_attn.bias\n",
      "transformer.h.9.attn.c_proj.weight\n",
      "transformer.h.9.attn.c_proj.bias\n",
      "transformer.h.9.ln_2.weight\n",
      "transformer.h.9.ln_2.bias\n",
      "transformer.h.9.mlp.c_fc.weight\n",
      "transformer.h.9.mlp.c_fc.bias\n",
      "transformer.h.9.mlp.c_proj.weight\n",
      "transformer.h.9.mlp.c_proj.bias\n",
      "transformer.h.10.ln_1.weight\n",
      "transformer.h.10.ln_1.bias\n",
      "transformer.h.10.attn.c_attn.weight\n",
      "transformer.h.10.attn.c_attn.bias\n",
      "transformer.h.10.attn.c_proj.weight\n",
      "transformer.h.10.attn.c_proj.bias\n",
      "transformer.h.10.ln_2.weight\n",
      "transformer.h.10.ln_2.bias\n",
      "transformer.h.10.mlp.c_fc.weight\n",
      "transformer.h.10.mlp.c_fc.bias\n",
      "transformer.h.10.mlp.c_proj.weight\n",
      "transformer.h.10.mlp.c_proj.bias\n",
      "transformer.h.11.ln_1.weight\n",
      "transformer.h.11.ln_1.bias\n",
      "transformer.h.11.attn.c_attn.weight\n",
      "transformer.h.11.attn.c_attn.bias\n",
      "transformer.h.11.attn.c_proj.weight\n",
      "transformer.h.11.attn.c_proj.bias\n",
      "transformer.h.11.ln_2.weight\n",
      "transformer.h.11.ln_2.bias\n",
      "transformer.h.11.mlp.c_fc.weight\n",
      "transformer.h.11.mlp.c_fc.bias\n",
      "transformer.h.11.mlp.c_proj.weight\n",
      "transformer.h.11.mlp.c_proj.bias\n",
      "transformer.h.12.ln_1.weight\n",
      "transformer.h.12.ln_1.bias\n",
      "transformer.h.12.attn.c_attn.weight\n",
      "transformer.h.12.attn.c_attn.bias\n",
      "transformer.h.12.attn.c_proj.weight\n",
      "transformer.h.12.attn.c_proj.bias\n",
      "transformer.h.12.ln_2.weight\n",
      "transformer.h.12.ln_2.bias\n",
      "transformer.h.12.mlp.c_fc.weight\n",
      "transformer.h.12.mlp.c_fc.bias\n",
      "transformer.h.12.mlp.c_proj.weight\n",
      "transformer.h.12.mlp.c_proj.bias\n",
      "transformer.h.13.ln_1.weight\n",
      "transformer.h.13.ln_1.bias\n",
      "transformer.h.13.attn.c_attn.weight\n",
      "transformer.h.13.attn.c_attn.bias\n",
      "transformer.h.13.attn.c_proj.weight\n",
      "transformer.h.13.attn.c_proj.bias\n",
      "transformer.h.13.ln_2.weight\n",
      "transformer.h.13.ln_2.bias\n",
      "transformer.h.13.mlp.c_fc.weight\n",
      "transformer.h.13.mlp.c_fc.bias\n",
      "transformer.h.13.mlp.c_proj.weight\n",
      "transformer.h.13.mlp.c_proj.bias\n",
      "transformer.h.14.ln_1.weight\n",
      "transformer.h.14.ln_1.bias\n",
      "transformer.h.14.attn.c_attn.weight\n",
      "transformer.h.14.attn.c_attn.bias\n",
      "transformer.h.14.attn.c_proj.weight\n",
      "transformer.h.14.attn.c_proj.bias\n",
      "transformer.h.14.ln_2.weight\n",
      "transformer.h.14.ln_2.bias\n",
      "transformer.h.14.mlp.c_fc.weight\n",
      "transformer.h.14.mlp.c_fc.bias\n",
      "transformer.h.14.mlp.c_proj.weight\n",
      "transformer.h.14.mlp.c_proj.bias\n",
      "transformer.h.15.ln_1.weight\n",
      "transformer.h.15.ln_1.bias\n",
      "transformer.h.15.attn.c_attn.weight\n",
      "transformer.h.15.attn.c_attn.bias\n",
      "transformer.h.15.attn.c_proj.weight\n",
      "transformer.h.15.attn.c_proj.bias\n",
      "transformer.h.15.ln_2.weight\n",
      "transformer.h.15.ln_2.bias\n",
      "transformer.h.15.mlp.c_fc.weight\n",
      "transformer.h.15.mlp.c_fc.bias\n",
      "transformer.h.15.mlp.c_proj.weight\n",
      "transformer.h.15.mlp.c_proj.bias\n",
      "transformer.h.16.ln_1.weight\n",
      "transformer.h.16.ln_1.bias\n",
      "transformer.h.16.attn.c_attn.weight\n",
      "transformer.h.16.attn.c_attn.bias\n",
      "transformer.h.16.attn.c_proj.weight\n",
      "transformer.h.16.attn.c_proj.bias\n",
      "transformer.h.16.ln_2.weight\n",
      "transformer.h.16.ln_2.bias\n",
      "transformer.h.16.mlp.c_fc.weight\n",
      "transformer.h.16.mlp.c_fc.bias\n",
      "transformer.h.16.mlp.c_proj.weight\n",
      "transformer.h.16.mlp.c_proj.bias\n",
      "transformer.h.17.ln_1.weight\n",
      "transformer.h.17.ln_1.bias\n",
      "transformer.h.17.attn.c_attn.weight\n",
      "transformer.h.17.attn.c_attn.bias\n",
      "transformer.h.17.attn.c_proj.weight\n",
      "transformer.h.17.attn.c_proj.bias\n",
      "transformer.h.17.ln_2.weight\n",
      "transformer.h.17.ln_2.bias\n",
      "transformer.h.17.mlp.c_fc.weight\n",
      "transformer.h.17.mlp.c_fc.bias\n",
      "transformer.h.17.mlp.c_proj.weight\n",
      "transformer.h.17.mlp.c_proj.bias\n",
      "transformer.h.18.ln_1.weight\n",
      "transformer.h.18.ln_1.bias\n",
      "transformer.h.18.attn.c_attn.weight\n",
      "transformer.h.18.attn.c_attn.bias\n",
      "transformer.h.18.attn.c_proj.weight\n",
      "transformer.h.18.attn.c_proj.bias\n",
      "transformer.h.18.ln_2.weight\n",
      "transformer.h.18.ln_2.bias\n",
      "transformer.h.18.mlp.c_fc.weight\n",
      "transformer.h.18.mlp.c_fc.bias\n",
      "transformer.h.18.mlp.c_proj.weight\n",
      "transformer.h.18.mlp.c_proj.bias\n",
      "transformer.h.19.ln_1.weight\n",
      "transformer.h.19.ln_1.bias\n",
      "transformer.h.19.attn.c_attn.weight\n",
      "transformer.h.19.attn.c_attn.bias\n",
      "transformer.h.19.attn.c_proj.weight\n",
      "transformer.h.19.attn.c_proj.bias\n",
      "transformer.h.19.ln_2.weight\n",
      "transformer.h.19.ln_2.bias\n",
      "transformer.h.19.mlp.c_fc.weight\n",
      "transformer.h.19.mlp.c_fc.bias\n",
      "transformer.h.19.mlp.c_proj.weight\n",
      "transformer.h.19.mlp.c_proj.bias\n",
      "transformer.h.20.ln_1.weight\n",
      "transformer.h.20.ln_1.bias\n",
      "transformer.h.20.attn.c_attn.weight\n",
      "transformer.h.20.attn.c_attn.bias\n",
      "transformer.h.20.attn.c_proj.weight\n",
      "transformer.h.20.attn.c_proj.bias\n",
      "transformer.h.20.ln_2.weight\n",
      "transformer.h.20.ln_2.bias\n",
      "transformer.h.20.mlp.c_fc.weight\n",
      "transformer.h.20.mlp.c_fc.bias\n",
      "transformer.h.20.mlp.c_proj.weight\n",
      "transformer.h.20.mlp.c_proj.bias\n",
      "transformer.h.21.ln_1.weight\n",
      "transformer.h.21.ln_1.bias\n",
      "transformer.h.21.attn.c_attn.weight\n",
      "transformer.h.21.attn.c_attn.bias\n",
      "transformer.h.21.attn.c_proj.weight\n",
      "transformer.h.21.attn.c_proj.bias\n",
      "transformer.h.21.ln_2.weight\n",
      "transformer.h.21.ln_2.bias\n",
      "transformer.h.21.mlp.c_fc.weight\n",
      "transformer.h.21.mlp.c_fc.bias\n",
      "transformer.h.21.mlp.c_proj.weight\n",
      "transformer.h.21.mlp.c_proj.bias\n",
      "transformer.h.22.ln_1.weight\n",
      "transformer.h.22.ln_1.bias\n",
      "transformer.h.22.attn.c_attn.weight\n",
      "transformer.h.22.attn.c_attn.bias\n",
      "transformer.h.22.attn.c_proj.weight\n",
      "transformer.h.22.attn.c_proj.bias\n",
      "transformer.h.22.ln_2.weight\n",
      "transformer.h.22.ln_2.bias\n",
      "transformer.h.22.mlp.c_fc.weight\n",
      "transformer.h.22.mlp.c_fc.bias\n",
      "transformer.h.22.mlp.c_proj.weight\n",
      "transformer.h.22.mlp.c_proj.bias\n",
      "transformer.h.23.ln_1.weight\n",
      "transformer.h.23.ln_1.bias\n",
      "transformer.h.23.attn.c_attn.weight\n",
      "transformer.h.23.attn.c_attn.bias\n",
      "transformer.h.23.attn.c_proj.weight\n",
      "transformer.h.23.attn.c_proj.bias\n",
      "transformer.h.23.ln_2.weight\n",
      "transformer.h.23.ln_2.bias\n",
      "transformer.h.23.mlp.c_fc.weight\n",
      "transformer.h.23.mlp.c_fc.bias\n",
      "transformer.h.23.mlp.c_proj.weight\n",
      "transformer.h.23.mlp.c_proj.bias\n",
      "transformer.ln_f.weight\n",
      "transformer.ln_f.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "797f9ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "760,300,032 total parameters.\n",
      "28,334,592 training parameters.\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    param.requires_grad = False\n",
    "    for template in ['.ln_f.', '.23.']:\n",
    "        if template in name: # choose whatever you like here\n",
    "            param.requires_grad = True\n",
    "        \n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"{total_params:,} total parameters.\")\n",
    "total_trainable_params = sum(\n",
    "    p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"{total_trainable_params:,} training parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97e93928",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3aa48f6eb6946d18deb9125c4b0a2fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/953 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8c91b3e30524e7fbcf39da952473699",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/990k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e0ade6df6fe4f3c890815a8af203163",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.27M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4b7baebddbc4b18aa39f26c354c77cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7881946d62ee4b22bfa764c1c0ed48e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/145 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('rccmsu/ruadapt_llama2_7b_v0.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e19d8628",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.actuals = []\n",
    "        self.actuals_f = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82aff32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, source_len, summ_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.source_len = source_len\n",
    "        self.summ_len = summ_len\n",
    "        self.text = self.data.text\n",
    "        self.ctext = self.data.ctext\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        ctext = str(self.ctext[index])\n",
    "        ctext = ' '.join(ctext.split())\n",
    "\n",
    "        text = str(self.text[index])\n",
    "        text = ' '.join(text.split())\n",
    "\n",
    "        source = self.tokenizer.batch_encode_plus([ctext], max_length=self.source_len,return_tensors='pt')\n",
    "        target = self.tokenizer.batch_encode_plus([text], max_length=self.summ_len, return_tensors='pt')\n",
    "\n",
    "        source_ids = source['input_ids'].squeeze()\n",
    "        source_mask = source['attention_mask'].squeeze()\n",
    "        target_ids = target['input_ids'].squeeze()\n",
    "        target_mask = target['attention_mask'].squeeze()\n",
    "\n",
    "        return {\n",
    "            'source_ids': source_ids.to(dtype=torch.long), \n",
    "            'source_mask': source_mask.to(dtype=torch.long), \n",
    "            'target_ids': target_ids.to(dtype=torch.long),\n",
    "            'target_ids_y': target_ids.to(dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb483007",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "def train(epoch, tokenizer, model, device, loader, optimizer):\n",
    "    model.train()\n",
    "    for _,data in enumerate(loader, 0):\n",
    "        y = data['target_ids'].to(device, dtype = torch.long)\n",
    "        y_ids = y[:, :-1].contiguous()\n",
    "        labels = y[:, 1:].clone().detach()\n",
    "        labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
    "        ids = data['source_ids'].to(device, dtype = torch.long)\n",
    "        mask = data['source_mask'].to(device, dtype = torch.long)\n",
    "        #outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, labels=labels)\n",
    "        outputs = model(input_ids = ids, attention_mask = mask, labels=labels)\n",
    "        loss = outputs[0]\n",
    "        \n",
    "        if _%500==0:\n",
    "            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29e2d4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "rouge = evaluate.load(\"rouge\")\n",
    " \n",
    "def compute_metrics(decoded_preds,decoded_labels):\n",
    "    result = rouge.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=decoded_labels,\n",
    "        use_stemmer=True,\n",
    "        rouge_types=[\n",
    "            'rouge1',\n",
    "            'rouge2',\n",
    "            'rougeL'\n",
    "        ]\n",
    "    )\n",
    " \n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06d50803",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(epoch, tokenizer, model, device, loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(loader, 0):\n",
    "            y = data['target_ids'].to(device, dtype = torch.long)\n",
    "            ids = data['source_ids'].to(device, dtype = torch.long)\n",
    "            mask = data['source_mask'].to(device, dtype = torch.long)\n",
    "\n",
    "            generated_ids = model.generate(\n",
    "                input_ids = ids,\n",
    "                attention_mask = mask, \n",
    "                max_length = config.SUMMARY_LEN\n",
    "                )\n",
    "            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "            predictions.extend(preds)\n",
    "            if not config.actuals_f:\n",
    "                target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]  \n",
    "                actuals.extend(target)\n",
    "        if not config.actuals_f:\n",
    "            config.actuals_f = True\n",
    "            config.actuals = actuals\n",
    "        scores = compute_metrics(predictions, config.actuals)\n",
    "        print(predictions[0])\n",
    "    return predictions, config.actuals, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5634807a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               ctext  \\\n",
      "0  <s>user\\nДепозит: 6300рПолная стоимость: 19700...   \n",
      "1  <s>user\\nОплата - 16495р2202 2063 4522 7990Сбе...   \n",
      "2  <s>user\\nэто может заблокировать и нашу и вашу...   \n",
      "3                              <s>user\\nждем:)</s>\\n   \n",
      "4                <s>user\\nПришлите чек оплаты)</s>\\n   \n",
      "\n",
      "                                                text  \n",
      "0              угу\\nздесь интересует вариант с двумя  \n",
      "1  у меня банк с приколом, там нельзя отправить б...  \n",
      "2  тогда пока что отмена, карты другого банка нем...  \n",
      "3  вроде нормально, на карту можно без сообщения ...  \n",
      "4  это подойдет или нужно где подробнее? (сумма с...  \n",
      "FULL Dataset: (17989, 2)\n",
      "TRAIN Dataset: (14391, 2)\n",
      "TEST Dataset: (3598, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "760,300,032 total parameters.\n",
      "760,300,032 training parameters.\n",
      "Initiating Fine-Tuning for the model on our dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f544a310ccd43aa8d90af4997bab7f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stack expects each tensor to be equal size, but got [10] at entry 0 and [7] at entry 1\n",
      "Now generating summaries on our fine tuned model for the validation dataset and saving it in a dataframe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 94\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNow generating summaries on our fine tuned model for the validation dataset and saving it in a dataframe\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mVAL_EPOCHS):\n\u001b[1;32m---> 94\u001b[0m     predictions, actuals \u001b[38;5;241m=\u001b[39m validate(epoch, tokenizer, model, device, val_loader)\n\u001b[0;32m     95\u001b[0m     final_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGenerated Text\u001b[39m\u001b[38;5;124m'\u001b[39m:predictions,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mActual Text\u001b[39m\u001b[38;5;124m'\u001b[39m:actuals})\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOutput Files generated for review\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[6], line 11\u001b[0m, in \u001b[0;36mvalidate\u001b[1;34m(epoch, tokenizer, model, device, loader)\u001b[0m\n\u001b[0;32m      8\u001b[0m ids \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[0;32m      9\u001b[0m mask \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[1;32m---> 11\u001b[0m generated_ids \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m     12\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m ids,\n\u001b[0;32m     13\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m mask, \n\u001b[0;32m     14\u001b[0m     max_length \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mSUMMARY_LEN\n\u001b[0;32m     15\u001b[0m     )\n\u001b[0;32m     16\u001b[0m preds \u001b[38;5;241m=\u001b[39m [tokenizer\u001b[38;5;241m.\u001b[39mdecode(g, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m generated_ids]\n\u001b[0;32m     17\u001b[0m predictions\u001b[38;5;241m.\u001b[39mextend(preds)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\generation\\utils.py:1544\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   1526\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massisted_decoding(\n\u001b[0;32m   1527\u001b[0m         input_ids,\n\u001b[0;32m   1528\u001b[0m         candidate_generator\u001b[38;5;241m=\u001b[39mcandidate_generator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1541\u001b[0m     )\n\u001b[0;32m   1542\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGREEDY_SEARCH:\n\u001b[0;32m   1543\u001b[0m     \u001b[38;5;66;03m# 11. run greedy search\u001b[39;00m\n\u001b[1;32m-> 1544\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgreedy_search(\n\u001b[0;32m   1545\u001b[0m         input_ids,\n\u001b[0;32m   1546\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[0;32m   1547\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[0;32m   1548\u001b[0m         pad_token_id\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mpad_token_id,\n\u001b[0;32m   1549\u001b[0m         eos_token_id\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39meos_token_id,\n\u001b[0;32m   1550\u001b[0m         output_scores\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39moutput_scores,\n\u001b[0;32m   1551\u001b[0m         output_logits\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39moutput_logits,\n\u001b[0;32m   1552\u001b[0m         return_dict_in_generate\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mreturn_dict_in_generate,\n\u001b[0;32m   1553\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[0;32m   1554\u001b[0m         streamer\u001b[38;5;241m=\u001b[39mstreamer,\n\u001b[0;32m   1555\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1556\u001b[0m     )\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[0;32m   1559\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\generation\\utils.py:2467\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   2464\u001b[0m         this_peer_finished \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   2466\u001b[0m \u001b[38;5;66;03m# stop if we exceed the maximum length\u001b[39;00m\n\u001b[1;32m-> 2467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stopping_criteria(input_ids, scores):\n\u001b[0;32m   2468\u001b[0m     this_peer_finished \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   2470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m this_peer_finished \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m synced_gpus:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\generation\\stopping_criteria.py:130\u001b[0m, in \u001b[0;36mStoppingCriteriaList.__call__\u001b[1;34m(self, input_ids, scores, **kwargs)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mStoppingCriteriaList\u001b[39;00m(\u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m--> 130\u001b[0m     \u001b[38;5;129m@add_start_docstrings\u001b[39m(STOPPING_CRITERIA_INPUTS_DOCSTRING)\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids: torch\u001b[38;5;241m.\u001b[39mLongTensor, scores: torch\u001b[38;5;241m.\u001b[39mFloatTensor, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m    132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28many\u001b[39m(criteria(input_ids, scores, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mfor\u001b[39;00m criteria \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmax_length\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mint\u001b[39m]:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "device='cuda:0'\n",
    "config = Config()         # Initialize config\n",
    "config.MODEL_NAME = 'ai-forever/rugpt3large_based_on_gpt2'\n",
    "config.TRAIN_BATCH_SIZE = 9    # input batch size for training (default: 64)\n",
    "config.VALID_BATCH_SIZE = 1    # input batch size for testing (default: 1000)\n",
    "config.TRAIN_EPOCHS = 100       # number of epochs to train (default: 10)\n",
    "config.VAL_EPOCHS = 1 \n",
    "config.LEARNING_RATE = 5e-5    # learning rate (default: 0.01)\n",
    "config.SEED = 42               # random seed (default: 42)\n",
    "config.MAX_LEN = 1024\n",
    "config.SUMMARY_LEN = 256 \n",
    "\n",
    "best = 0\n",
    "\n",
    "# Set random seeds and deterministic pytorch for reproducibility\n",
    "torch.manual_seed(config.SEED) # pytorch random seed\n",
    "np.random.seed(config.SEED) # numpy random seed\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# tokenzier for encoding the text\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(config.MODEL_NAME)\n",
    "\n",
    "df = pd.read_excel('conv_dataset.xlsx')\n",
    "df.columns = ['ctext', 'text']\n",
    "#df = df[['text','ctext']]\n",
    "df.ctext = '<s>user\\n' + df.ctext + '</s>\\n'\n",
    "print(df.head())\n",
    "train_size = 0.8\n",
    "train_dataset=df.sample(frac=train_size,random_state = config.SEED)\n",
    "val_dataset=df.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(df.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(val_dataset.shape))\n",
    "\n",
    "training_set = CustomDataset(train_dataset, tokenizer, config.MAX_LEN, config.SUMMARY_LEN)\n",
    "val_set = CustomDataset(val_dataset, tokenizer, config.MAX_LEN, config.SUMMARY_LEN)\n",
    "\n",
    "train_params = {\n",
    "    'batch_size': config.TRAIN_BATCH_SIZE,\n",
    "    'shuffle': True,\n",
    "    'num_workers': 0\n",
    "    }\n",
    "\n",
    "val_params = {\n",
    "    'batch_size': config.VALID_BATCH_SIZE,\n",
    "    'shuffle': False,\n",
    "    'num_workers': 0\n",
    "    }\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "val_loader = DataLoader(val_set, **val_params)\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(config.MODEL_NAME, device_map=device)\n",
    "#model = model.to(device)\n",
    "'''\n",
    "for name, param in model.named_parameters():\n",
    "    param.requires_grad = False\n",
    "    for template in ['lm_head']:\n",
    "        if template in name: # choose whatever you like here\n",
    "            param.requires_grad = True\n",
    "            \n",
    "'''\n",
    "        \n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"{total_params:,} total parameters.\")\n",
    "total_trainable_params = sum(\n",
    "    p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"{total_trainable_params:,} training parameters.\")\n",
    "\n",
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=config.LEARNING_RATE)\n",
    "\n",
    "#wandb.watch(model, log=\"all\")\n",
    "\n",
    "print('Initiating Fine-Tuning for the model on our dataset')\n",
    "try:\n",
    "    for epoch in tqdm(range(config.TRAIN_EPOCHS)):\n",
    "        train(epoch, tokenizer, model, device, training_loader, optimizer)\n",
    "        #if epoch % 10 == 0:\n",
    "        _, __, scores = validate(epoch, tokenizer, model, device, val_loader)\n",
    "        print(scores)\n",
    "        if scores['rouge2'] > best:\n",
    "            best = scores['rouge2']\n",
    "            model.save_pretrained(\"checkpoints//conv_rouge2_\"+str(best).split('.')[-1])\n",
    "            \n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "print('Now generating summaries on our fine tuned model for the validation dataset and saving it in a dataframe')\n",
    "for epoch in range(config.VAL_EPOCHS):\n",
    "    predictions, actuals = validate(epoch, tokenizer, model, device, val_loader)\n",
    "    final_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals})\n",
    "    print('Output Files generated for review')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5936f11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a39d15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd3ad86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb85738c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "\n",
    "MODEL_NAME = \"rccmsu/ruadapt_llama2_7b_v0.1\"\n",
    "DEFAULT_MESSAGE_TEMPLATE = \"<s>{role}\\n{content}</s>\\n\"\n",
    "DEFAULT_SYSTEM_PROMPT = \"ты — миша, русскоязычный парень 24 лет живущий в москве. ты разговариваешь с людьми и подшучиваешь над ними.\"\n",
    "DEFAULT_RESPONSE_TEMPLATE = \"<s>bot\\n\"\n",
    "\n",
    "class Conversation:\n",
    "    def __init__(\n",
    "        self,\n",
    "        message_template=DEFAULT_MESSAGE_TEMPLATE,\n",
    "        system_prompt=DEFAULT_SYSTEM_PROMPT,\n",
    "        response_template=DEFAULT_RESPONSE_TEMPLATE\n",
    "    ):\n",
    "        self.message_template = message_template\n",
    "        self.response_template = response_template\n",
    "        self.messages = [{\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_prompt\n",
    "        }]\n",
    "\n",
    "    def add_user_message(self, message):\n",
    "        self.messages.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": message\n",
    "        })\n",
    "\n",
    "    def add_bot_message(self, message):\n",
    "        self.messages.append({\n",
    "            \"role\": \"bot\",\n",
    "            \"content\": message\n",
    "        })\n",
    "\n",
    "    def get_prompt(self, tokenizer):\n",
    "        final_text = \"\"\n",
    "        for message in self.messages:\n",
    "            message_text = self.message_template.format(**message)\n",
    "            final_text += message_text\n",
    "        final_text += DEFAULT_RESPONSE_TEMPLATE\n",
    "        return final_text.strip()\n",
    "\n",
    "def generate(model, tokenizer, prompt, generation_config):\n",
    "    data = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    data = {k: v.to(model.device) for k, v in data.items()}\n",
    "    output_ids = model.generate(\n",
    "        **data,\n",
    "        max_length = 256,\n",
    "        generation_config=generation_config\n",
    "    )[0]\n",
    "    output_ids = output_ids[len(data[\"input_ids\"][0]):]\n",
    "    output = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "    return output.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d3c6879",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b3d4c29ce61437ab12a94071925cb96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"cuda:0\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "model.to('cuda:0')\n",
    "model.eval()\n",
    "\n",
    "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
    "print(generation_config)\n",
    "\n",
    "conversation = Conversation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa1c3862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<flask_cors.extension.CORS at 0x2181cb1cd50>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from flask import Flask, jsonify, request\n",
    "from flask_cors import CORS\n",
    "app = Flask(__name__)\n",
    "\n",
    "\n",
    "# Cross Origin Resource Sharing (CORS) handling\n",
    "CORS(app, resources={'/captcha': {\"origins\": \"http://localhost:8080\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d77b0595",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/ai_bot', methods=['POST'])\n",
    "def post_request():\n",
    "    conversation.add_user_message(request.json['user_input'])\n",
    "    prompt = conversation.get_prompt(tokenizer)\n",
    "\n",
    "    output = generate(model, tokenizer, prompt, generation_config)\n",
    "    return jsonify({'output':output})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e254dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:8080\n",
      " * Running on http://192.168.1.34:8080\n",
      "Press CTRL+C to quit\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:665: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "192.168.1.144 - - [28/Aug/2024 23:13:09] \"POST /ai_bot HTTP/1.1\" 200 -\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "192.168.1.144 - - [28/Aug/2024 23:14:01] \"POST /ai_bot HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "app.run(host='0.0.0.0', port=8080)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94655f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4c3446b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# the if-condition is adapted to devices on M1/M2\n",
    "device = 'cuda:0' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c80bbab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForLanguageModeling, TextDataset\n",
    "from transformers import AdamW, get_cosine_schedule_with_warmup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7167d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m GPT2Tokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name_or_path)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# loading a pre-trained model based on GPT2\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m model \u001b[38;5;241m=\u001b[39m GPT2LMHeadModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinetuned_model/checkpoint-5000\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "model_name_or_path = 'ai-forever/rugpt3large_based_on_gpt2'\n",
    "\n",
    "# tokenizer based on GPT2 for text preprocessing\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "# loading a pre-trained model based on GPT2\n",
    "model = GPT2LMHeadModel.from_pretrained('finetuned_model/checkpoint-5000').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0366637c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt, do_sample=True, num_beams=2, temperature=1, top_p=0.9, max_length=256):\n",
    "    \n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(input_ids, \n",
    "                            do_sample=do_sample,\n",
    "                            num_beams=num_beams,\n",
    "                            temperature=temperature,\n",
    "                            top_p=top_p,\n",
    "                            max_length=max_length,\n",
    "                            )\n",
    "\n",
    "    print(list(map(tokenizer.decode, out))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c225e99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "откуда у тебя такой рейтинг?<s>user\n",
      "ну типо того да\n",
      "но вообще отец был не очень доволен тем что я занялся этим как раз в то время\n",
      "ему не нравилось что я все время был на математике и почти ничего не делал дома\n",
      "он считал что это плохо и ничего не делает дома\n",
      "поэтому он хотел чтобы я занялся чем то типо фриланса\n",
      "ну типо там сидел бы дома и разбирался в проге\n",
      "а я в это время хотел шизить в свое хобби и рисовать аниме блядот\n",
      "поэтому нам приходилось совмещать приятное с полезным</s>\n"
     ]
    }
   ],
   "source": [
    "# generation example before fine-tuning\n",
    "generate('откуда у тебя такой рейтинг?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5306d874",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f07b399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               ctext  \\\n",
      "0  <s>user\\nДепозит: 6300рПолная стоимость: 19700...   \n",
      "1  <s>user\\nОплата - 16495р2202 2063 4522 7990Сбе...   \n",
      "2  <s>user\\nэто может заблокировать и нашу и вашу...   \n",
      "3                              <s>user\\nждем:)</s>\\n   \n",
      "4                <s>user\\nПришлите чек оплаты)</s>\\n   \n",
      "\n",
      "                                                text  \n",
      "0              угу\\nздесь интересует вариант с двумя  \n",
      "1  у меня банк с приколом, там нельзя отправить б...  \n",
      "2  тогда пока что отмена, карты другого банка нем...  \n",
      "3  вроде нормально, на карту можно без сообщения ...  \n",
      "4  это подойдет или нужно где подробнее? (сумма с...  \n",
      "FULL Dataset: (17989, 2)\n",
      "TRAIN Dataset: (14391, 2)\n",
      "TEST Dataset: (3598, 2)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel('conv_dataset.xlsx')\n",
    "df.columns = ['ctext', 'text']\n",
    "#df = df[['text','ctext']]\n",
    "df.ctext = '<s>user\\n' + df.ctext + '</s>\\n'\n",
    "print(df.head())\n",
    "train_size = 0.8\n",
    "train_dataset=df.sample(frac=train_size,random_state = 42)\n",
    "val_dataset=df.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(df.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(val_dataset.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3af8e05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "for i in range(train_dataset.shape[0]):\n",
    "    lines.append(train_dataset.loc[i, 'ctext'])\n",
    "    lines.append(train_dataset.loc[i, 'text'])\n",
    "with open('txt_dataset.txt', 'w', encoding='utf-8') as f:\n",
    "    f.writelines(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7d971b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TextDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m TextDataset(tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,file_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtxt_dataset.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m      2\u001b[0m                             block_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m)\n\u001b[0;32m      4\u001b[0m data_collator \u001b[38;5;241m=\u001b[39m DataCollatorForLanguageModeling(tokenizer\u001b[38;5;241m=\u001b[39mtokenizer, \n\u001b[0;32m      5\u001b[0m                                                 mlm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TextDataset' is not defined"
     ]
    }
   ],
   "source": [
    "train_dataset = TextDataset(tokenizer=tokenizer,file_path='txt_dataset.txt', \n",
    "                            block_size=64)\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, \n",
    "                                                mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ead2dcf1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_dataset\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86718b78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f81b9d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir = \"./finetuned_model\",\n",
    "    overwrite_output_dir = True,\n",
    "    num_train_epochs = 7,\n",
    "    gradient_accumulation_steps = 2,\n",
    "    fp16 = True,\n",
    "    per_device_train_batch_size = 8,\n",
    "    learning_rate = 0.0002,\n",
    "    optim = 'adafactor',\n",
    "    lr_scheduler_type = 'cosine'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da97376a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "228a3a87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5313' max='5313' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5313/5313 15:26, Epoch 7/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>4.146800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.566900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>3.136800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.144300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.776800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.357000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.888400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.744600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.595700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.496100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5313, training_loss=1.8030999713354183, metrics={'train_runtime': 927.1167, 'train_samples_per_second': 91.683, 'train_steps_per_second': 5.731, 'total_flos': 2.219414088174797e+16, 'train_loss': 1.8030999713354183, 'epoch': 7.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdf0d96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
