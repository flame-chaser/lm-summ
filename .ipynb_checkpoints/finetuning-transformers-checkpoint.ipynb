{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93324806",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "# Загружаем данные\n",
    "df = pd.read_excel('data.xlsx')\n",
    "df = df[['text','summary']]\n",
    "\n",
    "# Конвертируем датафрейм в Dataset\n",
    "train, test = train_test_split(df, test_size=0.1)\n",
    "train_, val_ = train_test_split(train, test_size=0.2)\n",
    "dataset_train = Dataset.from_pandas(train_)\n",
    "dataset_valid  = Dataset.from_pandas(val_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50bc3387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Материальные внеоборотные активы\t103.0\t343.0\t519.0\n",
      "Нематериальные, финансовые и другие внеоборотные активы\t174.0\tnan\tnan\n",
      "Запасы\t533.0\t153.0\t175.0\n",
      "Денежные средства и денежные эквиваленты\t8535.0\t20395.0\t7870.0\n",
      "Финансовые и другие оборотные активы\t51565.0\t8977.0\t14856.0\n",
      "БАЛАНС\t60910.0\t29868.0\t23420.0\n",
      "Капитал и резервы\t43152.0\t18339.0\t8894.0\n",
      "Целевые средства\tnan\tnan\tnan\n",
      "Фонд недвижимого и особо ценного движимого имущества и иные целевые фонды\tnan\tnan\tnan\n",
      "Долгосрочные заемные средства\tnan\tnan\tnan\n",
      "Другие долгосрочные обязательства\tnan\tnan\tnan\n",
      "Краткосрочные заемные средства\tnan\tnan\tnan\n",
      "Кредиторская задолженность\t17758.0\t11529.0\t14526.0\n",
      "Другие краткосрочные обязательства\tnan\tnan\tnan\n"
     ]
    }
   ],
   "source": [
    "print(df.loc[0, 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7936304b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import (\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    pipeline,\n",
    "    GPT2Tokenizer\n",
    ")\n",
    " \n",
    "import glob\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f443fc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(text, model, tokenizer, max_length=512, num_beams=5):\n",
    "    # Preprocess the text\n",
    "    inputs = tokenizer.tokenizer.encode(\n",
    "        \"<LM> Сократи текст.\\n \" + text,\n",
    "        return_tensors='pt',\n",
    "        max_length=max_length,\n",
    "        truncation=True\n",
    "    )\n",
    " \n",
    "    # Generate the summary\n",
    "    summary_ids = model.generate(\n",
    "        inputs.to('cuda:0'),\n",
    "        max_length=256,\n",
    "        num_beams=num_beams,\n",
    "        # early_stopping=True,\n",
    "    )\n",
    " \n",
    "    # Decode and return the summary\n",
    "    return tokenizer.tokenizer.decode(summary_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3734dee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 'RussianNLP/FRED-T5-Summarizer'\n",
    "BATCH_SIZE = 1\n",
    "NUM_PROCS = 1\n",
    "EPOCHS = 10\n",
    "OUT_DIR = 'results_RussianNLP/FRED-T5-Summarizer'\n",
    "MAX_LENGTH = 256 # Maximum context length to consider while preparing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "565619c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/228 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3892: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/57 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, T5ForConditionalGeneration, pipeline\n",
    "tokenizer_origin = GPT2Tokenizer.from_pretrained(MODEL)\n",
    " \n",
    "# Function to convert text data into model inputs and targets\n",
    "\n",
    "class TokenizerWrapper:\n",
    "    def __init__(self, tokenizer, max_length):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.MAX_LENGTH = max_length\n",
    "\n",
    "    def preprocess_function(self, examples):\n",
    "        inputs = [f\"summarize: {article}\" for article in examples['text']]\n",
    "        model_inputs = self.tokenizer(\n",
    "            inputs,\n",
    "            max_length=self.MAX_LENGTH,\n",
    "            truncation=True,\n",
    "            padding='max_length'\n",
    "        )\n",
    "\n",
    "        # Set up the tokenizer for targets\n",
    "        targets = [summary for summary in examples['summary']]\n",
    "        with self.tokenizer.as_target_tokenizer():\n",
    "            labels = self.tokenizer(\n",
    "                targets,\n",
    "                max_length=self.MAX_LENGTH,\n",
    "                truncation=True,\n",
    "                padding='max_length'\n",
    "            )\n",
    "\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        return model_inputs\n",
    "\n",
    "tokenizer = TokenizerWrapper(tokenizer_origin, MAX_LENGTH)\n",
    "# Apply the function to the whole dataset\n",
    "tokenized_train = dataset_train.map(\n",
    "    tokenizer.preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=NUM_PROCS\n",
    ")\n",
    "tokenized_valid = dataset_valid.map(\n",
    "    tokenizer.preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=NUM_PROCS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd2d22fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7aef15da7ba41a488c90df1d9bec4d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,740,354,048 total parameters.\n",
      "209,498,112 training parameters.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "model = T5ForConditionalGeneration.from_pretrained(MODEL)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "for name, param in model.named_parameters():\n",
    "    param.requires_grad = False\n",
    "    for template in ['lm_head', 'block.22.', 'block.23.', 'final_layer_norm']:\n",
    "        if template in name: # choose whatever you like here\n",
    "            param.requires_grad = True\n",
    "# Total parameters and trainable parameters.\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"{total_params:,} total parameters.\")\n",
    "total_trainable_params = sum(\n",
    "    p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"{total_trainable_params:,} training parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff1aa54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shared.weight\n",
      "encoder.block.0.layer.0.SelfAttention.q.weight\n",
      "encoder.block.0.layer.0.SelfAttention.k.weight\n",
      "encoder.block.0.layer.0.SelfAttention.v.weight\n",
      "encoder.block.0.layer.0.SelfAttention.o.weight\n",
      "encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight\n",
      "encoder.block.0.layer.0.layer_norm.weight\n",
      "encoder.block.0.layer.1.DenseReluDense.wi_0.weight\n",
      "encoder.block.0.layer.1.DenseReluDense.wi_1.weight\n",
      "encoder.block.0.layer.1.DenseReluDense.wo.weight\n",
      "encoder.block.0.layer.1.layer_norm.weight\n",
      "encoder.block.1.layer.0.SelfAttention.q.weight\n",
      "encoder.block.1.layer.0.SelfAttention.k.weight\n",
      "encoder.block.1.layer.0.SelfAttention.v.weight\n",
      "encoder.block.1.layer.0.SelfAttention.o.weight\n",
      "encoder.block.1.layer.0.layer_norm.weight\n",
      "encoder.block.1.layer.1.DenseReluDense.wi_0.weight\n",
      "encoder.block.1.layer.1.DenseReluDense.wi_1.weight\n",
      "encoder.block.1.layer.1.DenseReluDense.wo.weight\n",
      "encoder.block.1.layer.1.layer_norm.weight\n",
      "encoder.block.2.layer.0.SelfAttention.q.weight\n",
      "encoder.block.2.layer.0.SelfAttention.k.weight\n",
      "encoder.block.2.layer.0.SelfAttention.v.weight\n",
      "encoder.block.2.layer.0.SelfAttention.o.weight\n",
      "encoder.block.2.layer.0.layer_norm.weight\n",
      "encoder.block.2.layer.1.DenseReluDense.wi_0.weight\n",
      "encoder.block.2.layer.1.DenseReluDense.wi_1.weight\n",
      "encoder.block.2.layer.1.DenseReluDense.wo.weight\n",
      "encoder.block.2.layer.1.layer_norm.weight\n",
      "encoder.block.3.layer.0.SelfAttention.q.weight\n",
      "encoder.block.3.layer.0.SelfAttention.k.weight\n",
      "encoder.block.3.layer.0.SelfAttention.v.weight\n",
      "encoder.block.3.layer.0.SelfAttention.o.weight\n",
      "encoder.block.3.layer.0.layer_norm.weight\n",
      "encoder.block.3.layer.1.DenseReluDense.wi_0.weight\n",
      "encoder.block.3.layer.1.DenseReluDense.wi_1.weight\n",
      "encoder.block.3.layer.1.DenseReluDense.wo.weight\n",
      "encoder.block.3.layer.1.layer_norm.weight\n",
      "encoder.block.4.layer.0.SelfAttention.q.weight\n",
      "encoder.block.4.layer.0.SelfAttention.k.weight\n",
      "encoder.block.4.layer.0.SelfAttention.v.weight\n",
      "encoder.block.4.layer.0.SelfAttention.o.weight\n",
      "encoder.block.4.layer.0.layer_norm.weight\n",
      "encoder.block.4.layer.1.DenseReluDense.wi_0.weight\n",
      "encoder.block.4.layer.1.DenseReluDense.wi_1.weight\n",
      "encoder.block.4.layer.1.DenseReluDense.wo.weight\n",
      "encoder.block.4.layer.1.layer_norm.weight\n",
      "encoder.block.5.layer.0.SelfAttention.q.weight\n",
      "encoder.block.5.layer.0.SelfAttention.k.weight\n",
      "encoder.block.5.layer.0.SelfAttention.v.weight\n",
      "encoder.block.5.layer.0.SelfAttention.o.weight\n",
      "encoder.block.5.layer.0.layer_norm.weight\n",
      "encoder.block.5.layer.1.DenseReluDense.wi_0.weight\n",
      "encoder.block.5.layer.1.DenseReluDense.wi_1.weight\n",
      "encoder.block.5.layer.1.DenseReluDense.wo.weight\n",
      "encoder.block.5.layer.1.layer_norm.weight\n",
      "encoder.block.6.layer.0.SelfAttention.q.weight\n",
      "encoder.block.6.layer.0.SelfAttention.k.weight\n",
      "encoder.block.6.layer.0.SelfAttention.v.weight\n",
      "encoder.block.6.layer.0.SelfAttention.o.weight\n",
      "encoder.block.6.layer.0.layer_norm.weight\n",
      "encoder.block.6.layer.1.DenseReluDense.wi_0.weight\n",
      "encoder.block.6.layer.1.DenseReluDense.wi_1.weight\n",
      "encoder.block.6.layer.1.DenseReluDense.wo.weight\n",
      "encoder.block.6.layer.1.layer_norm.weight\n",
      "encoder.block.7.layer.0.SelfAttention.q.weight\n",
      "encoder.block.7.layer.0.SelfAttention.k.weight\n",
      "encoder.block.7.layer.0.SelfAttention.v.weight\n",
      "encoder.block.7.layer.0.SelfAttention.o.weight\n",
      "encoder.block.7.layer.0.layer_norm.weight\n",
      "encoder.block.7.layer.1.DenseReluDense.wi_0.weight\n",
      "encoder.block.7.layer.1.DenseReluDense.wi_1.weight\n",
      "encoder.block.7.layer.1.DenseReluDense.wo.weight\n",
      "encoder.block.7.layer.1.layer_norm.weight\n",
      "encoder.block.8.layer.0.SelfAttention.q.weight\n",
      "encoder.block.8.layer.0.SelfAttention.k.weight\n",
      "encoder.block.8.layer.0.SelfAttention.v.weight\n",
      "encoder.block.8.layer.0.SelfAttention.o.weight\n",
      "encoder.block.8.layer.0.layer_norm.weight\n",
      "encoder.block.8.layer.1.DenseReluDense.wi_0.weight\n",
      "encoder.block.8.layer.1.DenseReluDense.wi_1.weight\n",
      "encoder.block.8.layer.1.DenseReluDense.wo.weight\n",
      "encoder.block.8.layer.1.layer_norm.weight\n",
      "encoder.block.9.layer.0.SelfAttention.q.weight\n",
      "encoder.block.9.layer.0.SelfAttention.k.weight\n",
      "encoder.block.9.layer.0.SelfAttention.v.weight\n",
      "encoder.block.9.layer.0.SelfAttention.o.weight\n",
      "encoder.block.9.layer.0.layer_norm.weight\n",
      "encoder.block.9.layer.1.DenseReluDense.wi_0.weight\n",
      "encoder.block.9.layer.1.DenseReluDense.wi_1.weight\n",
      "encoder.block.9.layer.1.DenseReluDense.wo.weight\n",
      "encoder.block.9.layer.1.layer_norm.weight\n",
      "encoder.block.10.layer.0.SelfAttention.q.weight\n",
      "encoder.block.10.layer.0.SelfAttention.k.weight\n",
      "encoder.block.10.layer.0.SelfAttention.v.weight\n",
      "encoder.block.10.layer.0.SelfAttention.o.weight\n",
      "encoder.block.10.layer.0.layer_norm.weight\n",
      "encoder.block.10.layer.1.DenseReluDense.wi_0.weight\n",
      "encoder.block.10.layer.1.DenseReluDense.wi_1.weight\n",
      "encoder.block.10.layer.1.DenseReluDense.wo.weight\n",
      "encoder.block.10.layer.1.layer_norm.weight\n",
      "encoder.block.11.layer.0.SelfAttention.q.weight\n",
      "encoder.block.11.layer.0.SelfAttention.k.weight\n",
      "encoder.block.11.layer.0.SelfAttention.v.weight\n",
      "encoder.block.11.layer.0.SelfAttention.o.weight\n",
      "encoder.block.11.layer.0.layer_norm.weight\n",
      "encoder.block.11.layer.1.DenseReluDense.wi_0.weight\n",
      "encoder.block.11.layer.1.DenseReluDense.wi_1.weight\n",
      "encoder.block.11.layer.1.DenseReluDense.wo.weight\n",
      "encoder.block.11.layer.1.layer_norm.weight\n",
      "encoder.block.12.layer.0.SelfAttention.q.weight\n",
      "encoder.block.12.layer.0.SelfAttention.k.weight\n",
      "encoder.block.12.layer.0.SelfAttention.v.weight\n",
      "encoder.block.12.layer.0.SelfAttention.o.weight\n",
      "encoder.block.12.layer.0.layer_norm.weight\n",
      "encoder.block.12.layer.1.DenseReluDense.wi_0.weight\n",
      "encoder.block.12.layer.1.DenseReluDense.wi_1.weight\n",
      "encoder.block.12.layer.1.DenseReluDense.wo.weight\n",
      "encoder.block.12.layer.1.layer_norm.weight\n",
      "encoder.block.13.layer.0.SelfAttention.q.weight\n",
      "encoder.block.13.layer.0.SelfAttention.k.weight\n",
      "encoder.block.13.layer.0.SelfAttention.v.weight\n",
      "encoder.block.13.layer.0.SelfAttention.o.weight\n",
      "encoder.block.13.layer.0.layer_norm.weight\n",
      "encoder.block.13.layer.1.DenseReluDense.wi_0.weight\n",
      "encoder.block.13.layer.1.DenseReluDense.wi_1.weight\n",
      "encoder.block.13.layer.1.DenseReluDense.wo.weight\n",
      "encoder.block.13.layer.1.layer_norm.weight\n",
      "encoder.block.14.layer.0.SelfAttention.q.weight\n",
      "encoder.block.14.layer.0.SelfAttention.k.weight\n",
      "encoder.block.14.layer.0.SelfAttention.v.weight\n",
      "encoder.block.14.layer.0.SelfAttention.o.weight\n",
      "encoder.block.14.layer.0.layer_norm.weight\n",
      "encoder.block.14.layer.1.DenseReluDense.wi_0.weight\n",
      "encoder.block.14.layer.1.DenseReluDense.wi_1.weight\n",
      "encoder.block.14.layer.1.DenseReluDense.wo.weight\n",
      "encoder.block.14.layer.1.layer_norm.weight\n",
      "encoder.block.15.layer.0.SelfAttention.q.weight\n",
      "encoder.block.15.layer.0.SelfAttention.k.weight\n",
      "encoder.block.15.layer.0.SelfAttention.v.weight\n",
      "encoder.block.15.layer.0.SelfAttention.o.weight\n",
      "encoder.block.15.layer.0.layer_norm.weight\n",
      "encoder.block.15.layer.1.DenseReluDense.wi_0.weight\n",
      "encoder.block.15.layer.1.DenseReluDense.wi_1.weight\n",
      "encoder.block.15.layer.1.DenseReluDense.wo.weight\n",
      "encoder.block.15.layer.1.layer_norm.weight\n",
      "encoder.block.16.layer.0.SelfAttention.q.weight\n",
      "encoder.block.16.layer.0.SelfAttention.k.weight\n",
      "encoder.block.16.layer.0.SelfAttention.v.weight\n",
      "encoder.block.16.layer.0.SelfAttention.o.weight\n",
      "encoder.block.16.layer.0.layer_norm.weight\n",
      "encoder.block.16.layer.1.DenseReluDense.wi_0.weight\n",
      "encoder.block.16.layer.1.DenseReluDense.wi_1.weight\n",
      "encoder.block.16.layer.1.DenseReluDense.wo.weight\n",
      "encoder.block.16.layer.1.layer_norm.weight\n",
      "encoder.block.17.layer.0.SelfAttention.q.weight\n",
      "encoder.block.17.layer.0.SelfAttention.k.weight\n",
      "encoder.block.17.layer.0.SelfAttention.v.weight\n",
      "encoder.block.17.layer.0.SelfAttention.o.weight\n",
      "encoder.block.17.layer.0.layer_norm.weight\n",
      "encoder.block.17.layer.1.DenseReluDense.wi_0.weight\n",
      "encoder.block.17.layer.1.DenseReluDense.wi_1.weight\n",
      "encoder.block.17.layer.1.DenseReluDense.wo.weight\n",
      "encoder.block.17.layer.1.layer_norm.weight\n",
      "encoder.block.18.layer.0.SelfAttention.q.weight\n",
      "encoder.block.18.layer.0.SelfAttention.k.weight\n",
      "encoder.block.18.layer.0.SelfAttention.v.weight\n",
      "encoder.block.18.layer.0.SelfAttention.o.weight\n",
      "encoder.block.18.layer.0.layer_norm.weight\n",
      "encoder.block.18.layer.1.DenseReluDense.wi_0.weight\n",
      "encoder.block.18.layer.1.DenseReluDense.wi_1.weight\n",
      "encoder.block.18.layer.1.DenseReluDense.wo.weight\n",
      "encoder.block.18.layer.1.layer_norm.weight\n",
      "encoder.block.19.layer.0.SelfAttention.q.weight\n",
      "encoder.block.19.layer.0.SelfAttention.k.weight\n",
      "encoder.block.19.layer.0.SelfAttention.v.weight\n",
      "encoder.block.19.layer.0.SelfAttention.o.weight\n",
      "encoder.block.19.layer.0.layer_norm.weight\n",
      "encoder.block.19.layer.1.DenseReluDense.wi_0.weight\n",
      "encoder.block.19.layer.1.DenseReluDense.wi_1.weight\n",
      "encoder.block.19.layer.1.DenseReluDense.wo.weight\n",
      "encoder.block.19.layer.1.layer_norm.weight\n",
      "encoder.block.20.layer.0.SelfAttention.q.weight\n",
      "encoder.block.20.layer.0.SelfAttention.k.weight\n",
      "encoder.block.20.layer.0.SelfAttention.v.weight\n",
      "encoder.block.20.layer.0.SelfAttention.o.weight\n",
      "encoder.block.20.layer.0.layer_norm.weight\n",
      "encoder.block.20.layer.1.DenseReluDense.wi_0.weight\n",
      "encoder.block.20.layer.1.DenseReluDense.wi_1.weight\n",
      "encoder.block.20.layer.1.DenseReluDense.wo.weight\n",
      "encoder.block.20.layer.1.layer_norm.weight\n",
      "encoder.block.21.layer.0.SelfAttention.q.weight\n",
      "encoder.block.21.layer.0.SelfAttention.k.weight\n",
      "encoder.block.21.layer.0.SelfAttention.v.weight\n",
      "encoder.block.21.layer.0.SelfAttention.o.weight\n",
      "encoder.block.21.layer.0.layer_norm.weight\n",
      "encoder.block.21.layer.1.DenseReluDense.wi_0.weight\n",
      "encoder.block.21.layer.1.DenseReluDense.wi_1.weight\n",
      "encoder.block.21.layer.1.DenseReluDense.wo.weight\n",
      "encoder.block.21.layer.1.layer_norm.weight\n",
      "encoder.block.22.layer.0.SelfAttention.q.weight\n",
      "encoder.block.22.layer.0.SelfAttention.k.weight\n",
      "encoder.block.22.layer.0.SelfAttention.v.weight\n",
      "encoder.block.22.layer.0.SelfAttention.o.weight\n",
      "encoder.block.22.layer.0.layer_norm.weight\n",
      "encoder.block.22.layer.1.DenseReluDense.wi_0.weight\n",
      "encoder.block.22.layer.1.DenseReluDense.wi_1.weight\n",
      "encoder.block.22.layer.1.DenseReluDense.wo.weight\n",
      "encoder.block.22.layer.1.layer_norm.weight\n",
      "encoder.block.23.layer.0.SelfAttention.q.weight\n",
      "encoder.block.23.layer.0.SelfAttention.k.weight\n",
      "encoder.block.23.layer.0.SelfAttention.v.weight\n",
      "encoder.block.23.layer.0.SelfAttention.o.weight\n",
      "encoder.block.23.layer.0.layer_norm.weight\n",
      "encoder.block.23.layer.1.DenseReluDense.wi_0.weight\n",
      "encoder.block.23.layer.1.DenseReluDense.wi_1.weight\n",
      "encoder.block.23.layer.1.DenseReluDense.wo.weight\n",
      "encoder.block.23.layer.1.layer_norm.weight\n",
      "encoder.final_layer_norm.weight\n",
      "decoder.block.0.layer.0.SelfAttention.q.weight\n",
      "decoder.block.0.layer.0.SelfAttention.k.weight\n",
      "decoder.block.0.layer.0.SelfAttention.v.weight\n",
      "decoder.block.0.layer.0.SelfAttention.o.weight\n",
      "decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight\n",
      "decoder.block.0.layer.0.layer_norm.weight\n",
      "decoder.block.0.layer.1.EncDecAttention.q.weight\n",
      "decoder.block.0.layer.1.EncDecAttention.k.weight\n",
      "decoder.block.0.layer.1.EncDecAttention.v.weight\n",
      "decoder.block.0.layer.1.EncDecAttention.o.weight\n",
      "decoder.block.0.layer.1.layer_norm.weight\n",
      "decoder.block.0.layer.2.DenseReluDense.wi_0.weight\n",
      "decoder.block.0.layer.2.DenseReluDense.wi_1.weight\n",
      "decoder.block.0.layer.2.DenseReluDense.wo.weight\n",
      "decoder.block.0.layer.2.layer_norm.weight\n",
      "decoder.block.1.layer.0.SelfAttention.q.weight\n",
      "decoder.block.1.layer.0.SelfAttention.k.weight\n",
      "decoder.block.1.layer.0.SelfAttention.v.weight\n",
      "decoder.block.1.layer.0.SelfAttention.o.weight\n",
      "decoder.block.1.layer.0.layer_norm.weight\n",
      "decoder.block.1.layer.1.EncDecAttention.q.weight\n",
      "decoder.block.1.layer.1.EncDecAttention.k.weight\n",
      "decoder.block.1.layer.1.EncDecAttention.v.weight\n",
      "decoder.block.1.layer.1.EncDecAttention.o.weight\n",
      "decoder.block.1.layer.1.layer_norm.weight\n",
      "decoder.block.1.layer.2.DenseReluDense.wi_0.weight\n",
      "decoder.block.1.layer.2.DenseReluDense.wi_1.weight\n",
      "decoder.block.1.layer.2.DenseReluDense.wo.weight\n",
      "decoder.block.1.layer.2.layer_norm.weight\n",
      "decoder.block.2.layer.0.SelfAttention.q.weight\n",
      "decoder.block.2.layer.0.SelfAttention.k.weight\n",
      "decoder.block.2.layer.0.SelfAttention.v.weight\n",
      "decoder.block.2.layer.0.SelfAttention.o.weight\n",
      "decoder.block.2.layer.0.layer_norm.weight\n",
      "decoder.block.2.layer.1.EncDecAttention.q.weight\n",
      "decoder.block.2.layer.1.EncDecAttention.k.weight\n",
      "decoder.block.2.layer.1.EncDecAttention.v.weight\n",
      "decoder.block.2.layer.1.EncDecAttention.o.weight\n",
      "decoder.block.2.layer.1.layer_norm.weight\n",
      "decoder.block.2.layer.2.DenseReluDense.wi_0.weight\n",
      "decoder.block.2.layer.2.DenseReluDense.wi_1.weight\n",
      "decoder.block.2.layer.2.DenseReluDense.wo.weight\n",
      "decoder.block.2.layer.2.layer_norm.weight\n",
      "decoder.block.3.layer.0.SelfAttention.q.weight\n",
      "decoder.block.3.layer.0.SelfAttention.k.weight\n",
      "decoder.block.3.layer.0.SelfAttention.v.weight\n",
      "decoder.block.3.layer.0.SelfAttention.o.weight\n",
      "decoder.block.3.layer.0.layer_norm.weight\n",
      "decoder.block.3.layer.1.EncDecAttention.q.weight\n",
      "decoder.block.3.layer.1.EncDecAttention.k.weight\n",
      "decoder.block.3.layer.1.EncDecAttention.v.weight\n",
      "decoder.block.3.layer.1.EncDecAttention.o.weight\n",
      "decoder.block.3.layer.1.layer_norm.weight\n",
      "decoder.block.3.layer.2.DenseReluDense.wi_0.weight\n",
      "decoder.block.3.layer.2.DenseReluDense.wi_1.weight\n",
      "decoder.block.3.layer.2.DenseReluDense.wo.weight\n",
      "decoder.block.3.layer.2.layer_norm.weight\n",
      "decoder.block.4.layer.0.SelfAttention.q.weight\n",
      "decoder.block.4.layer.0.SelfAttention.k.weight\n",
      "decoder.block.4.layer.0.SelfAttention.v.weight\n",
      "decoder.block.4.layer.0.SelfAttention.o.weight\n",
      "decoder.block.4.layer.0.layer_norm.weight\n",
      "decoder.block.4.layer.1.EncDecAttention.q.weight\n",
      "decoder.block.4.layer.1.EncDecAttention.k.weight\n",
      "decoder.block.4.layer.1.EncDecAttention.v.weight\n",
      "decoder.block.4.layer.1.EncDecAttention.o.weight\n",
      "decoder.block.4.layer.1.layer_norm.weight\n",
      "decoder.block.4.layer.2.DenseReluDense.wi_0.weight\n",
      "decoder.block.4.layer.2.DenseReluDense.wi_1.weight\n",
      "decoder.block.4.layer.2.DenseReluDense.wo.weight\n",
      "decoder.block.4.layer.2.layer_norm.weight\n",
      "decoder.block.5.layer.0.SelfAttention.q.weight\n",
      "decoder.block.5.layer.0.SelfAttention.k.weight\n",
      "decoder.block.5.layer.0.SelfAttention.v.weight\n",
      "decoder.block.5.layer.0.SelfAttention.o.weight\n",
      "decoder.block.5.layer.0.layer_norm.weight\n",
      "decoder.block.5.layer.1.EncDecAttention.q.weight\n",
      "decoder.block.5.layer.1.EncDecAttention.k.weight\n",
      "decoder.block.5.layer.1.EncDecAttention.v.weight\n",
      "decoder.block.5.layer.1.EncDecAttention.o.weight\n",
      "decoder.block.5.layer.1.layer_norm.weight\n",
      "decoder.block.5.layer.2.DenseReluDense.wi_0.weight\n",
      "decoder.block.5.layer.2.DenseReluDense.wi_1.weight\n",
      "decoder.block.5.layer.2.DenseReluDense.wo.weight\n",
      "decoder.block.5.layer.2.layer_norm.weight\n",
      "decoder.block.6.layer.0.SelfAttention.q.weight\n",
      "decoder.block.6.layer.0.SelfAttention.k.weight\n",
      "decoder.block.6.layer.0.SelfAttention.v.weight\n",
      "decoder.block.6.layer.0.SelfAttention.o.weight\n",
      "decoder.block.6.layer.0.layer_norm.weight\n",
      "decoder.block.6.layer.1.EncDecAttention.q.weight\n",
      "decoder.block.6.layer.1.EncDecAttention.k.weight\n",
      "decoder.block.6.layer.1.EncDecAttention.v.weight\n",
      "decoder.block.6.layer.1.EncDecAttention.o.weight\n",
      "decoder.block.6.layer.1.layer_norm.weight\n",
      "decoder.block.6.layer.2.DenseReluDense.wi_0.weight\n",
      "decoder.block.6.layer.2.DenseReluDense.wi_1.weight\n",
      "decoder.block.6.layer.2.DenseReluDense.wo.weight\n",
      "decoder.block.6.layer.2.layer_norm.weight\n",
      "decoder.block.7.layer.0.SelfAttention.q.weight\n",
      "decoder.block.7.layer.0.SelfAttention.k.weight\n",
      "decoder.block.7.layer.0.SelfAttention.v.weight\n",
      "decoder.block.7.layer.0.SelfAttention.o.weight\n",
      "decoder.block.7.layer.0.layer_norm.weight\n",
      "decoder.block.7.layer.1.EncDecAttention.q.weight\n",
      "decoder.block.7.layer.1.EncDecAttention.k.weight\n",
      "decoder.block.7.layer.1.EncDecAttention.v.weight\n",
      "decoder.block.7.layer.1.EncDecAttention.o.weight\n",
      "decoder.block.7.layer.1.layer_norm.weight\n",
      "decoder.block.7.layer.2.DenseReluDense.wi_0.weight\n",
      "decoder.block.7.layer.2.DenseReluDense.wi_1.weight\n",
      "decoder.block.7.layer.2.DenseReluDense.wo.weight\n",
      "decoder.block.7.layer.2.layer_norm.weight\n",
      "decoder.block.8.layer.0.SelfAttention.q.weight\n",
      "decoder.block.8.layer.0.SelfAttention.k.weight\n",
      "decoder.block.8.layer.0.SelfAttention.v.weight\n",
      "decoder.block.8.layer.0.SelfAttention.o.weight\n",
      "decoder.block.8.layer.0.layer_norm.weight\n",
      "decoder.block.8.layer.1.EncDecAttention.q.weight\n",
      "decoder.block.8.layer.1.EncDecAttention.k.weight\n",
      "decoder.block.8.layer.1.EncDecAttention.v.weight\n",
      "decoder.block.8.layer.1.EncDecAttention.o.weight\n",
      "decoder.block.8.layer.1.layer_norm.weight\n",
      "decoder.block.8.layer.2.DenseReluDense.wi_0.weight\n",
      "decoder.block.8.layer.2.DenseReluDense.wi_1.weight\n",
      "decoder.block.8.layer.2.DenseReluDense.wo.weight\n",
      "decoder.block.8.layer.2.layer_norm.weight\n",
      "decoder.block.9.layer.0.SelfAttention.q.weight\n",
      "decoder.block.9.layer.0.SelfAttention.k.weight\n",
      "decoder.block.9.layer.0.SelfAttention.v.weight\n",
      "decoder.block.9.layer.0.SelfAttention.o.weight\n",
      "decoder.block.9.layer.0.layer_norm.weight\n",
      "decoder.block.9.layer.1.EncDecAttention.q.weight\n",
      "decoder.block.9.layer.1.EncDecAttention.k.weight\n",
      "decoder.block.9.layer.1.EncDecAttention.v.weight\n",
      "decoder.block.9.layer.1.EncDecAttention.o.weight\n",
      "decoder.block.9.layer.1.layer_norm.weight\n",
      "decoder.block.9.layer.2.DenseReluDense.wi_0.weight\n",
      "decoder.block.9.layer.2.DenseReluDense.wi_1.weight\n",
      "decoder.block.9.layer.2.DenseReluDense.wo.weight\n",
      "decoder.block.9.layer.2.layer_norm.weight\n",
      "decoder.block.10.layer.0.SelfAttention.q.weight\n",
      "decoder.block.10.layer.0.SelfAttention.k.weight\n",
      "decoder.block.10.layer.0.SelfAttention.v.weight\n",
      "decoder.block.10.layer.0.SelfAttention.o.weight\n",
      "decoder.block.10.layer.0.layer_norm.weight\n",
      "decoder.block.10.layer.1.EncDecAttention.q.weight\n",
      "decoder.block.10.layer.1.EncDecAttention.k.weight\n",
      "decoder.block.10.layer.1.EncDecAttention.v.weight\n",
      "decoder.block.10.layer.1.EncDecAttention.o.weight\n",
      "decoder.block.10.layer.1.layer_norm.weight\n",
      "decoder.block.10.layer.2.DenseReluDense.wi_0.weight\n",
      "decoder.block.10.layer.2.DenseReluDense.wi_1.weight\n",
      "decoder.block.10.layer.2.DenseReluDense.wo.weight\n",
      "decoder.block.10.layer.2.layer_norm.weight\n",
      "decoder.block.11.layer.0.SelfAttention.q.weight\n",
      "decoder.block.11.layer.0.SelfAttention.k.weight\n",
      "decoder.block.11.layer.0.SelfAttention.v.weight\n",
      "decoder.block.11.layer.0.SelfAttention.o.weight\n",
      "decoder.block.11.layer.0.layer_norm.weight\n",
      "decoder.block.11.layer.1.EncDecAttention.q.weight\n",
      "decoder.block.11.layer.1.EncDecAttention.k.weight\n",
      "decoder.block.11.layer.1.EncDecAttention.v.weight\n",
      "decoder.block.11.layer.1.EncDecAttention.o.weight\n",
      "decoder.block.11.layer.1.layer_norm.weight\n",
      "decoder.block.11.layer.2.DenseReluDense.wi_0.weight\n",
      "decoder.block.11.layer.2.DenseReluDense.wi_1.weight\n",
      "decoder.block.11.layer.2.DenseReluDense.wo.weight\n",
      "decoder.block.11.layer.2.layer_norm.weight\n",
      "decoder.block.12.layer.0.SelfAttention.q.weight\n",
      "decoder.block.12.layer.0.SelfAttention.k.weight\n",
      "decoder.block.12.layer.0.SelfAttention.v.weight\n",
      "decoder.block.12.layer.0.SelfAttention.o.weight\n",
      "decoder.block.12.layer.0.layer_norm.weight\n",
      "decoder.block.12.layer.1.EncDecAttention.q.weight\n",
      "decoder.block.12.layer.1.EncDecAttention.k.weight\n",
      "decoder.block.12.layer.1.EncDecAttention.v.weight\n",
      "decoder.block.12.layer.1.EncDecAttention.o.weight\n",
      "decoder.block.12.layer.1.layer_norm.weight\n",
      "decoder.block.12.layer.2.DenseReluDense.wi_0.weight\n",
      "decoder.block.12.layer.2.DenseReluDense.wi_1.weight\n",
      "decoder.block.12.layer.2.DenseReluDense.wo.weight\n",
      "decoder.block.12.layer.2.layer_norm.weight\n",
      "decoder.block.13.layer.0.SelfAttention.q.weight\n",
      "decoder.block.13.layer.0.SelfAttention.k.weight\n",
      "decoder.block.13.layer.0.SelfAttention.v.weight\n",
      "decoder.block.13.layer.0.SelfAttention.o.weight\n",
      "decoder.block.13.layer.0.layer_norm.weight\n",
      "decoder.block.13.layer.1.EncDecAttention.q.weight\n",
      "decoder.block.13.layer.1.EncDecAttention.k.weight\n",
      "decoder.block.13.layer.1.EncDecAttention.v.weight\n",
      "decoder.block.13.layer.1.EncDecAttention.o.weight\n",
      "decoder.block.13.layer.1.layer_norm.weight\n",
      "decoder.block.13.layer.2.DenseReluDense.wi_0.weight\n",
      "decoder.block.13.layer.2.DenseReluDense.wi_1.weight\n",
      "decoder.block.13.layer.2.DenseReluDense.wo.weight\n",
      "decoder.block.13.layer.2.layer_norm.weight\n",
      "decoder.block.14.layer.0.SelfAttention.q.weight\n",
      "decoder.block.14.layer.0.SelfAttention.k.weight\n",
      "decoder.block.14.layer.0.SelfAttention.v.weight\n",
      "decoder.block.14.layer.0.SelfAttention.o.weight\n",
      "decoder.block.14.layer.0.layer_norm.weight\n",
      "decoder.block.14.layer.1.EncDecAttention.q.weight\n",
      "decoder.block.14.layer.1.EncDecAttention.k.weight\n",
      "decoder.block.14.layer.1.EncDecAttention.v.weight\n",
      "decoder.block.14.layer.1.EncDecAttention.o.weight\n",
      "decoder.block.14.layer.1.layer_norm.weight\n",
      "decoder.block.14.layer.2.DenseReluDense.wi_0.weight\n",
      "decoder.block.14.layer.2.DenseReluDense.wi_1.weight\n",
      "decoder.block.14.layer.2.DenseReluDense.wo.weight\n",
      "decoder.block.14.layer.2.layer_norm.weight\n",
      "decoder.block.15.layer.0.SelfAttention.q.weight\n",
      "decoder.block.15.layer.0.SelfAttention.k.weight\n",
      "decoder.block.15.layer.0.SelfAttention.v.weight\n",
      "decoder.block.15.layer.0.SelfAttention.o.weight\n",
      "decoder.block.15.layer.0.layer_norm.weight\n",
      "decoder.block.15.layer.1.EncDecAttention.q.weight\n",
      "decoder.block.15.layer.1.EncDecAttention.k.weight\n",
      "decoder.block.15.layer.1.EncDecAttention.v.weight\n",
      "decoder.block.15.layer.1.EncDecAttention.o.weight\n",
      "decoder.block.15.layer.1.layer_norm.weight\n",
      "decoder.block.15.layer.2.DenseReluDense.wi_0.weight\n",
      "decoder.block.15.layer.2.DenseReluDense.wi_1.weight\n",
      "decoder.block.15.layer.2.DenseReluDense.wo.weight\n",
      "decoder.block.15.layer.2.layer_norm.weight\n",
      "decoder.block.16.layer.0.SelfAttention.q.weight\n",
      "decoder.block.16.layer.0.SelfAttention.k.weight\n",
      "decoder.block.16.layer.0.SelfAttention.v.weight\n",
      "decoder.block.16.layer.0.SelfAttention.o.weight\n",
      "decoder.block.16.layer.0.layer_norm.weight\n",
      "decoder.block.16.layer.1.EncDecAttention.q.weight\n",
      "decoder.block.16.layer.1.EncDecAttention.k.weight\n",
      "decoder.block.16.layer.1.EncDecAttention.v.weight\n",
      "decoder.block.16.layer.1.EncDecAttention.o.weight\n",
      "decoder.block.16.layer.1.layer_norm.weight\n",
      "decoder.block.16.layer.2.DenseReluDense.wi_0.weight\n",
      "decoder.block.16.layer.2.DenseReluDense.wi_1.weight\n",
      "decoder.block.16.layer.2.DenseReluDense.wo.weight\n",
      "decoder.block.16.layer.2.layer_norm.weight\n",
      "decoder.block.17.layer.0.SelfAttention.q.weight\n",
      "decoder.block.17.layer.0.SelfAttention.k.weight\n",
      "decoder.block.17.layer.0.SelfAttention.v.weight\n",
      "decoder.block.17.layer.0.SelfAttention.o.weight\n",
      "decoder.block.17.layer.0.layer_norm.weight\n",
      "decoder.block.17.layer.1.EncDecAttention.q.weight\n",
      "decoder.block.17.layer.1.EncDecAttention.k.weight\n",
      "decoder.block.17.layer.1.EncDecAttention.v.weight\n",
      "decoder.block.17.layer.1.EncDecAttention.o.weight\n",
      "decoder.block.17.layer.1.layer_norm.weight\n",
      "decoder.block.17.layer.2.DenseReluDense.wi_0.weight\n",
      "decoder.block.17.layer.2.DenseReluDense.wi_1.weight\n",
      "decoder.block.17.layer.2.DenseReluDense.wo.weight\n",
      "decoder.block.17.layer.2.layer_norm.weight\n",
      "decoder.block.18.layer.0.SelfAttention.q.weight\n",
      "decoder.block.18.layer.0.SelfAttention.k.weight\n",
      "decoder.block.18.layer.0.SelfAttention.v.weight\n",
      "decoder.block.18.layer.0.SelfAttention.o.weight\n",
      "decoder.block.18.layer.0.layer_norm.weight\n",
      "decoder.block.18.layer.1.EncDecAttention.q.weight\n",
      "decoder.block.18.layer.1.EncDecAttention.k.weight\n",
      "decoder.block.18.layer.1.EncDecAttention.v.weight\n",
      "decoder.block.18.layer.1.EncDecAttention.o.weight\n",
      "decoder.block.18.layer.1.layer_norm.weight\n",
      "decoder.block.18.layer.2.DenseReluDense.wi_0.weight\n",
      "decoder.block.18.layer.2.DenseReluDense.wi_1.weight\n",
      "decoder.block.18.layer.2.DenseReluDense.wo.weight\n",
      "decoder.block.18.layer.2.layer_norm.weight\n",
      "decoder.block.19.layer.0.SelfAttention.q.weight\n",
      "decoder.block.19.layer.0.SelfAttention.k.weight\n",
      "decoder.block.19.layer.0.SelfAttention.v.weight\n",
      "decoder.block.19.layer.0.SelfAttention.o.weight\n",
      "decoder.block.19.layer.0.layer_norm.weight\n",
      "decoder.block.19.layer.1.EncDecAttention.q.weight\n",
      "decoder.block.19.layer.1.EncDecAttention.k.weight\n",
      "decoder.block.19.layer.1.EncDecAttention.v.weight\n",
      "decoder.block.19.layer.1.EncDecAttention.o.weight\n",
      "decoder.block.19.layer.1.layer_norm.weight\n",
      "decoder.block.19.layer.2.DenseReluDense.wi_0.weight\n",
      "decoder.block.19.layer.2.DenseReluDense.wi_1.weight\n",
      "decoder.block.19.layer.2.DenseReluDense.wo.weight\n",
      "decoder.block.19.layer.2.layer_norm.weight\n",
      "decoder.block.20.layer.0.SelfAttention.q.weight\n",
      "decoder.block.20.layer.0.SelfAttention.k.weight\n",
      "decoder.block.20.layer.0.SelfAttention.v.weight\n",
      "decoder.block.20.layer.0.SelfAttention.o.weight\n",
      "decoder.block.20.layer.0.layer_norm.weight\n",
      "decoder.block.20.layer.1.EncDecAttention.q.weight\n",
      "decoder.block.20.layer.1.EncDecAttention.k.weight\n",
      "decoder.block.20.layer.1.EncDecAttention.v.weight\n",
      "decoder.block.20.layer.1.EncDecAttention.o.weight\n",
      "decoder.block.20.layer.1.layer_norm.weight\n",
      "decoder.block.20.layer.2.DenseReluDense.wi_0.weight\n",
      "decoder.block.20.layer.2.DenseReluDense.wi_1.weight\n",
      "decoder.block.20.layer.2.DenseReluDense.wo.weight\n",
      "decoder.block.20.layer.2.layer_norm.weight\n",
      "decoder.block.21.layer.0.SelfAttention.q.weight\n",
      "decoder.block.21.layer.0.SelfAttention.k.weight\n",
      "decoder.block.21.layer.0.SelfAttention.v.weight\n",
      "decoder.block.21.layer.0.SelfAttention.o.weight\n",
      "decoder.block.21.layer.0.layer_norm.weight\n",
      "decoder.block.21.layer.1.EncDecAttention.q.weight\n",
      "decoder.block.21.layer.1.EncDecAttention.k.weight\n",
      "decoder.block.21.layer.1.EncDecAttention.v.weight\n",
      "decoder.block.21.layer.1.EncDecAttention.o.weight\n",
      "decoder.block.21.layer.1.layer_norm.weight\n",
      "decoder.block.21.layer.2.DenseReluDense.wi_0.weight\n",
      "decoder.block.21.layer.2.DenseReluDense.wi_1.weight\n",
      "decoder.block.21.layer.2.DenseReluDense.wo.weight\n",
      "decoder.block.21.layer.2.layer_norm.weight\n",
      "decoder.block.22.layer.0.SelfAttention.q.weight\n",
      "decoder.block.22.layer.0.SelfAttention.k.weight\n",
      "decoder.block.22.layer.0.SelfAttention.v.weight\n",
      "decoder.block.22.layer.0.SelfAttention.o.weight\n",
      "decoder.block.22.layer.0.layer_norm.weight\n",
      "decoder.block.22.layer.1.EncDecAttention.q.weight\n",
      "decoder.block.22.layer.1.EncDecAttention.k.weight\n",
      "decoder.block.22.layer.1.EncDecAttention.v.weight\n",
      "decoder.block.22.layer.1.EncDecAttention.o.weight\n",
      "decoder.block.22.layer.1.layer_norm.weight\n",
      "decoder.block.22.layer.2.DenseReluDense.wi_0.weight\n",
      "decoder.block.22.layer.2.DenseReluDense.wi_1.weight\n",
      "decoder.block.22.layer.2.DenseReluDense.wo.weight\n",
      "decoder.block.22.layer.2.layer_norm.weight\n",
      "decoder.block.23.layer.0.SelfAttention.q.weight\n",
      "decoder.block.23.layer.0.SelfAttention.k.weight\n",
      "decoder.block.23.layer.0.SelfAttention.v.weight\n",
      "decoder.block.23.layer.0.SelfAttention.o.weight\n",
      "decoder.block.23.layer.0.layer_norm.weight\n",
      "decoder.block.23.layer.1.EncDecAttention.q.weight\n",
      "decoder.block.23.layer.1.EncDecAttention.k.weight\n",
      "decoder.block.23.layer.1.EncDecAttention.v.weight\n",
      "decoder.block.23.layer.1.EncDecAttention.o.weight\n",
      "decoder.block.23.layer.1.layer_norm.weight\n",
      "decoder.block.23.layer.2.DenseReluDense.wi_0.weight\n",
      "decoder.block.23.layer.2.DenseReluDense.wi_1.weight\n",
      "decoder.block.23.layer.2.DenseReluDense.wo.weight\n",
      "decoder.block.23.layer.2.layer_norm.weight\n",
      "decoder.final_layer_norm.weight\n",
      "lm_head.weight\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78708b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load(\"rouge\")\n",
    " \n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred.predictions[0], eval_pred.label_ids\n",
    " \n",
    "    decoded_preds = tokenizer.tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    " \n",
    "    result = rouge.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=decoded_labels,\n",
    "        use_stemmer=True,\n",
    "        rouge_types=[\n",
    "            'rouge1',\n",
    "            'rouge2',\n",
    "            'rougeL'\n",
    "        ]\n",
    "    )\n",
    " \n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    " \n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41b04a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    \"\"\"\n",
    "    Original Trainer may have a memory leak.\n",
    "    This is a workaround to avoid storing too many tensors that are not needed.\n",
    "    \"\"\"\n",
    "    pred_ids = torch.argmax(logits[0], dim=-1)\n",
    "    return pred_ids, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb029c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\flame-chaser\\AppData\\Roaming\\Python\\Python311\\site-packages\\accelerate\\accelerator.py:451: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2280' max='2280' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2280/2280 08:47, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>13.546900</td>\n",
       "      <td>5.396227</td>\n",
       "      <td>0.144400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.140400</td>\n",
       "      <td>22.122800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.178300</td>\n",
       "      <td>0.035610</td>\n",
       "      <td>0.263200</td>\n",
       "      <td>0.035100</td>\n",
       "      <td>0.263200</td>\n",
       "      <td>20.017500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.028800</td>\n",
       "      <td>0.023927</td>\n",
       "      <td>0.254400</td>\n",
       "      <td>0.035100</td>\n",
       "      <td>0.254400</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.024300</td>\n",
       "      <td>0.023694</td>\n",
       "      <td>0.228100</td>\n",
       "      <td>0.035100</td>\n",
       "      <td>0.228100</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.023900</td>\n",
       "      <td>0.022007</td>\n",
       "      <td>0.263200</td>\n",
       "      <td>0.052600</td>\n",
       "      <td>0.263200</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.023600</td>\n",
       "      <td>0.021530</td>\n",
       "      <td>0.254400</td>\n",
       "      <td>0.052600</td>\n",
       "      <td>0.254400</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.020800</td>\n",
       "      <td>0.021382</td>\n",
       "      <td>0.254400</td>\n",
       "      <td>0.052600</td>\n",
       "      <td>0.254400</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.126200</td>\n",
       "      <td>0.021631</td>\n",
       "      <td>0.298200</td>\n",
       "      <td>0.052600</td>\n",
       "      <td>0.289500</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.021900</td>\n",
       "      <td>0.021674</td>\n",
       "      <td>0.254400</td>\n",
       "      <td>0.052600</td>\n",
       "      <td>0.245600</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.022300</td>\n",
       "      <td>0.021007</td>\n",
       "      <td>0.307000</td>\n",
       "      <td>0.052600</td>\n",
       "      <td>0.298200</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.022200</td>\n",
       "      <td>0.020825</td>\n",
       "      <td>0.307000</td>\n",
       "      <td>0.070200</td>\n",
       "      <td>0.298200</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=OUT_DIR,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=OUT_DIR,\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=200,\n",
    "    save_strategy='epoch',\n",
    "    save_total_limit=2,\n",
    "    report_to='tensorboard',\n",
    "    learning_rate=0.0001,\n",
    "    dataloader_num_workers=4\n",
    ")\n",
    "\n",
    "        \n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_valid,\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    " \n",
    "history = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399d330d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train val test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c7887bdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Компания имеет положительный тренд по 3 показателям и отрицательный по 1, в целом положение компании улучшается.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize_text(test.loc[295, 'text'], trainer.model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0cf19402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Компания имеет положительный тренд по 1 показателям и отрицательный по 9, в целом положение компании ухудшается.\n"
     ]
    }
   ],
   "source": [
    "print(test.loc[295, 'summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a3702ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>Нематериальные активы\\t0.0\\tnan\\t61.0\\nРезульт...</td>\n",
       "      <td>Компания имеет положительный тренд по 1 показа...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Материальные внеоборотные активы\\t159.0\\t350.0...</td>\n",
       "      <td>Компания имеет положительный тренд по 1 показа...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>Материальные внеоборотные активы\\t187.0\\t253.0...</td>\n",
       "      <td>Компания имеет положительный тренд по 1 показа...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>Результаты исследований и разработок\\tnan\\tnan...</td>\n",
       "      <td>Компания имеет положительный тренд по 0 показа...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>Нематериальные активы\\tnan\\tnan\\tnan\\nРезульта...</td>\n",
       "      <td>Компания имеет положительный тренд по 0 показа...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>Нематериальные активы\\t18928.0\\t13712.0\\t12673...</td>\n",
       "      <td>Компания имеет положительный тренд по 11 показ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>Нематериальные активы\\t13662.0\\t16688.0\\t16910...</td>\n",
       "      <td>Компания имеет положительный тренд по 4 показа...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>Нематериальные активы\\t217648.0\\t126907.0\\t700...</td>\n",
       "      <td>Компания имеет положительный тренд по 8 показа...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>Материальные внеоборотные активы\\tnan\\tnan\\tna...</td>\n",
       "      <td>Компания имеет положительный тренд по 0 показа...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>Нематериальные активы\\t7374.0\\t538.0\\t967.0\\nР...</td>\n",
       "      <td>Компания имеет положительный тренд по 3 показа...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>Нематериальные активы\\tnan\\tnan\\tnan\\nРезульта...</td>\n",
       "      <td>Компания имеет положительный тренд по 4 показа...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>Нематериальные активы\\t60848.0\\t72005.0\\t94371...</td>\n",
       "      <td>Компания имеет положительный тренд по 3 показа...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>Нематериальные активы\\tnan\\tnan\\tnan\\nРезульта...</td>\n",
       "      <td>Компания имеет положительный тренд по 5 показа...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>Результаты исследований и разработок\\tnan\\tnan...</td>\n",
       "      <td>Компания имеет положительный тренд по 1 показа...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>Результаты исследований и разработок\\tnan\\tnan...</td>\n",
       "      <td>Компания имеет положительный тренд по 7 показа...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>Нематериальные активы\\t16609.0\\tnan\\tnan\\nРезу...</td>\n",
       "      <td>Компания имеет положительный тренд по 2 показа...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>Нематериальные активы\\t1508.0\\t2345.0\\t3352.0\\...</td>\n",
       "      <td>Компания имеет положительный тренд по 7 показа...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>Нематериальные активы\\t0.0\\tnan\\t4.0\\nРезульта...</td>\n",
       "      <td>Компания имеет положительный тренд по 4 показа...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>Нематериальные активы\\t119006.0\\t104396.0\\t673...</td>\n",
       "      <td>Компания имеет положительный тренд по 8 показа...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>Нематериальные активы\\t231.0\\t260.0\\t289.0\\nРе...</td>\n",
       "      <td>Компания имеет положительный тренд по 5 показа...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>Нематериальные активы\\t32963.0\\t967.0\\t1693.0\\...</td>\n",
       "      <td>Компания имеет положительный тренд по 8 показа...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>1.\\t58381.0\\t98669.0\\t64200.0\\nРезультаты иссл...</td>\n",
       "      <td>Компания имеет положительный тренд по 9 показа...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Материальные внеоборотные активы\\t821.0\\t1000....</td>\n",
       "      <td>Компания имеет положительный тренд по 0 показа...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>Материальные внеоборотные активы\\t0.0\\tnan\\t35...</td>\n",
       "      <td>Компания имеет положительный тренд по 1 показа...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>Нематериальные активы\\t66636.0\\t30397.0\\t12380...</td>\n",
       "      <td>Компания имеет положительный тренд по 7 показа...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>Нематериальные активы\\tnan\\tnan\\tnan\\nРезульта...</td>\n",
       "      <td>Компания имеет положительный тренд по 1 показа...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Материальные внеоборотные активы\\t2221.0\\t4590...</td>\n",
       "      <td>Компания имеет положительный тренд по 1 показа...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>Нематериальные активы\\t59.0\\t90.0\\t120.0\\nРезу...</td>\n",
       "      <td>Компания имеет положительный тренд по 1 показа...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>Материальные внеоборотные активы\\t8.0\\t23.0\\t3...</td>\n",
       "      <td>Компания имеет положительный тренд по 0 показа...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>Нематериальные активы\\t0.0\\t4753.0\\t5466.0\\nРе...</td>\n",
       "      <td>Компания имеет положительный тренд по 0 показа...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Материальные внеоборотные активы\\t11833.0\\t126...</td>\n",
       "      <td>Компания имеет положительный тренд по 0 показа...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>Нематериальные активы\\tnan\\tnan\\tnan\\nРезульта...</td>\n",
       "      <td>Компания имеет положительный тренд по 5 показа...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "295  Нематериальные активы\\t0.0\\tnan\\t61.0\\nРезульт...   \n",
       "36   Материальные внеоборотные активы\\t159.0\\t350.0...   \n",
       "87   Материальные внеоборотные активы\\t187.0\\t253.0...   \n",
       "228  Результаты исследований и разработок\\tnan\\tnan...   \n",
       "141  Нематериальные активы\\tnan\\tnan\\tnan\\nРезульта...   \n",
       "296  Нематериальные активы\\t18928.0\\t13712.0\\t12673...   \n",
       "217  Нематериальные активы\\t13662.0\\t16688.0\\t16910...   \n",
       "170  Нематериальные активы\\t217648.0\\t126907.0\\t700...   \n",
       "102  Материальные внеоборотные активы\\tnan\\tnan\\tna...   \n",
       "147  Нематериальные активы\\t7374.0\\t538.0\\t967.0\\nР...   \n",
       "250  Нематериальные активы\\tnan\\tnan\\tnan\\nРезульта...   \n",
       "264  Нематериальные активы\\t60848.0\\t72005.0\\t94371...   \n",
       "220  Нематериальные активы\\tnan\\tnan\\tnan\\nРезульта...   \n",
       "230  Результаты исследований и разработок\\tnan\\tnan...   \n",
       "127  Результаты исследований и разработок\\tnan\\tnan...   \n",
       "201  Нематериальные активы\\t16609.0\\tnan\\tnan\\nРезу...   \n",
       "257  Нематериальные активы\\t1508.0\\t2345.0\\t3352.0\\...   \n",
       "297  Нематериальные активы\\t0.0\\tnan\\t4.0\\nРезульта...   \n",
       "218  Нематериальные активы\\t119006.0\\t104396.0\\t673...   \n",
       "233  Нематериальные активы\\t231.0\\t260.0\\t289.0\\nРе...   \n",
       "283  Нематериальные активы\\t32963.0\\t967.0\\t1693.0\\...   \n",
       "275  1.\\t58381.0\\t98669.0\\t64200.0\\nРезультаты иссл...   \n",
       "34   Материальные внеоборотные активы\\t821.0\\t1000....   \n",
       "108  Материальные внеоборотные активы\\t0.0\\tnan\\t35...   \n",
       "308  Нематериальные активы\\t66636.0\\t30397.0\\t12380...   \n",
       "208  Нематериальные активы\\tnan\\tnan\\tnan\\nРезульта...   \n",
       "46   Материальные внеоборотные активы\\t2221.0\\t4590...   \n",
       "299  Нематериальные активы\\t59.0\\t90.0\\t120.0\\nРезу...   \n",
       "117  Материальные внеоборотные активы\\t8.0\\t23.0\\t3...   \n",
       "142  Нематериальные активы\\t0.0\\t4753.0\\t5466.0\\nРе...   \n",
       "1    Материальные внеоборотные активы\\t11833.0\\t126...   \n",
       "245  Нематериальные активы\\tnan\\tnan\\tnan\\nРезульта...   \n",
       "\n",
       "                                               summary  \n",
       "295  Компания имеет положительный тренд по 1 показа...  \n",
       "36   Компания имеет положительный тренд по 1 показа...  \n",
       "87   Компания имеет положительный тренд по 1 показа...  \n",
       "228  Компания имеет положительный тренд по 0 показа...  \n",
       "141  Компания имеет положительный тренд по 0 показа...  \n",
       "296  Компания имеет положительный тренд по 11 показ...  \n",
       "217  Компания имеет положительный тренд по 4 показа...  \n",
       "170  Компания имеет положительный тренд по 8 показа...  \n",
       "102  Компания имеет положительный тренд по 0 показа...  \n",
       "147  Компания имеет положительный тренд по 3 показа...  \n",
       "250  Компания имеет положительный тренд по 4 показа...  \n",
       "264  Компания имеет положительный тренд по 3 показа...  \n",
       "220  Компания имеет положительный тренд по 5 показа...  \n",
       "230  Компания имеет положительный тренд по 1 показа...  \n",
       "127  Компания имеет положительный тренд по 7 показа...  \n",
       "201  Компания имеет положительный тренд по 2 показа...  \n",
       "257  Компания имеет положительный тренд по 7 показа...  \n",
       "297  Компания имеет положительный тренд по 4 показа...  \n",
       "218  Компания имеет положительный тренд по 8 показа...  \n",
       "233  Компания имеет положительный тренд по 5 показа...  \n",
       "283  Компания имеет положительный тренд по 8 показа...  \n",
       "275  Компания имеет положительный тренд по 9 показа...  \n",
       "34   Компания имеет положительный тренд по 0 показа...  \n",
       "108  Компания имеет положительный тренд по 1 показа...  \n",
       "308  Компания имеет положительный тренд по 7 показа...  \n",
       "208  Компания имеет положительный тренд по 1 показа...  \n",
       "46   Компания имеет положительный тренд по 1 показа...  \n",
       "299  Компания имеет положительный тренд по 1 показа...  \n",
       "117  Компания имеет положительный тренд по 0 показа...  \n",
       "142  Компания имеет положительный тренд по 0 показа...  \n",
       "1    Компания имеет положительный тренд по 0 показа...  \n",
       "245  Компания имеет положительный тренд по 5 показа...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2d84bf41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Компания имеет положительный тренд по 2 показателям и отрицательный по 1, в целом положение компании улучшается.\n"
     ]
    }
   ],
   "source": [
    "print(df.loc[0, 'summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f1d1abef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Компания имеет положительный тренд по 2 показателям и отрицательный по 1, в целом положение компании улучшается.\n"
     ]
    }
   ],
   "source": [
    "print(df.loc[0, 'summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fef79f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Материальные внеоборотные активы\t103.0\t343.0\t519.0\n",
      "Нематериальные, финансовые и другие внеоборотные активы\t174.0\tnan\tnan\n",
      "Запасы\t533.0\t153.0\t175.0\n",
      "Денежные средства и денежные эквиваленты\t8535.0\t20395.0\t7870.0\n",
      "Финансовые и другие оборотные активы\t51565.0\t8977.0\t14856.0\n",
      "БАЛАНС\t60910.0\t29868.0\t23420.0\n",
      "Капитал и резервы\t43152.0\t18339.0\t8894.0\n",
      "Целевые средства\tnan\tnan\tnan\n",
      "Фонд недвижимого и особо ценного движимого имущества и иные целевые фонды\tnan\tnan\tnan\n",
      "Долгосрочные заемные средства\tnan\tnan\tnan\n",
      "Другие долгосрочные обязательства\tnan\tnan\tnan\n",
      "Краткосрочные заемные средства\tnan\tnan\tnan\n",
      "Кредиторская задолженность\t17758.0\t11529.0\t14526.0\n",
      "Другие краткосрочные обязательства\tnan\tnan\tnan\n"
     ]
    }
   ],
   "source": [
    "print(df.loc[0, 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dad798d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
